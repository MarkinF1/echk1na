# Учебный проект Echk1na
Основная суть: \
    Есть различные насосы, на которых стоят датчики, считывающие различные параметры \
    такие как вибро - перемещение, скорость и ускорение. Они также имеют 3 различных направления. \
    Необходимо написать нейронную сеть, которая позволит заранее определять превышение каких-либо \
    параметров. \

Реализация: \
    Предложена реализация двух простых нейронных сетей: LSTM и полносвязная. \
    Сама модель состоит из 9 нейросетей, которые обучаются для предсказания отдельных \
    пар (unit, direction). Выборка для обучения будет происходить следующим образом: \
    выбирается модель конкретной пары для обучения, получает все дни, которые есть в датасете, \
    из них выделяются дни, на которые мы можем предсказать, то есть которые содержат данные в дни \
    от 'текущий - дни для предсказания - дни для анализа' до 'текущий - дни для предсказания' для пары этой модели. \
    Далее они деляться на тренировочные, валидацию и тестовые.
    
Реализованные полезности: \
    1) При запуске программы на обучение, если она не находит дамп данных, то при создании массивов \
    для тренир., валид. и тестов, она сохраняет их дамп для будущего использования. \
    2) С помощью конфигов можно достаточно точно настроить многие параметры такие как: \
    отключить конкретную пару (unit, direction), выбрать для загрузки сохраненную модель, \
    выбрать коэфициенты тренировки и теста, выбрать функцию потерь, оптимизатор, модель и еще многое другое.
    3)


Заметки: \
    * Данные очень плохие, валидных дней очень мало для параметров a10, p3, очень разряжены, \
    в некоторых парах около 10к всего данных, а в неоторых полмиллиона. \
    * Исходя из выше сказанного, первоначальная идея с использованием любых данных для подачи X_tensor \
    плохая, потому что в некоторых случаях есть какие-то пары, а внекоторых только пары определенного вида \
    (например пара (1, 2) содержит полмиллиона значений и полностью заполняет некоторые тензоры).
    * Оптимизатор SGD отрабатывает значительно хуже Adam на некоторых парах (unit, direction), \
    поэтому принято решение отказаться от него. \
    * Полносвязной модели нужен значительно больше шаг lr, чем lstm, тогда она будет лучше обучаться. \
    * Разницы между lstm с 10к слоями и 2к нет, ошибка примерно такая же, а обучается гораздо дольше. \
    * В данных бинарного вида есть тензоры со значениями -inf, это произошло скорее всего из-за \
    нормализации тензора. Там проискходит деление (скорее всего на ноль) и получается бесконечность. \
    Добавлена проверка на содержание бесконечности.
    